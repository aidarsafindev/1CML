#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
–î–∞–Ω–Ω—ã–µ: –∏–∑ Windows Performance Counters –∏–ª–∏ —Ç–µ—Ö–∂—É—Ä–Ω–∞–ª–∞ 1–°
–í–µ—Ä—Å–∏—è: 2.0
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime, timedelta
import logging
import os
import sys
from dotenv import load_dotenv
import argparse
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Path(__file__).parent / '../logs/disk_predict.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('disk_predictor')

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(Path(__file__).parent / '../.env')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'monitoring'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', 'password')
}

DISK_LIMIT_GB = float(os.getenv('DISK_LIMIT_GB', '200'))
DISK_LETTER = os.getenv('DISK_LETTER', 'D:')
FORECAST_DAYS = [7, 14, 30]  # –Ω–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º

def get_historical_disk_usage(days=60, source='test'):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–∏—Å–∫—É
    
    Args:
        days: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏
        source: –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ('test', 'prometheus', 'windows')
    
    Returns:
        DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ date, used_gb
    """
    if source == 'test':
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ
        logger.info("–ò—Å–ø–æ–ª—å–∑—É—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        # –õ–∏–Ω–µ–π–Ω—ã–π —Ä–æ—Å—Ç –æ—Ç 100 –¥–æ 150 –ì–ë —Å –Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º
        used = np.linspace(100, 150, 30) + np.random.normal(0, 2, 30)
        return pd.DataFrame({'date': dates, 'used_gb': used})
    
    elif source == 'prometheus':
        # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ Prometheus
        logger.error("–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ Prometheus –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ")
        sys.exit(1)
    
    elif source == 'windows':
        # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ WMI/Performance Counters
        logger.error("–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ Windows –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ")
        sys.exit(1)
    
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –ë–î
        try:
            conn = psycopg2.connect(**DB_CONFIG)
            query = """
                SELECT 
                    date, 
                    used_gb 
                FROM disk_usage 
                WHERE date >= CURRENT_DATE - INTERVAL '%s days'
                ORDER BY date
            """ % days
            
            df = pd.read_sql(query, conn, parse_dates=['date'])
            conn.close()
            
            if df.empty:
                logger.warning("–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î. –ò—Å–ø–æ–ª—å–∑—É—é —Ç–µ—Å—Ç–æ–≤—ã–µ.")
                return get_historical_disk_usage(days, 'test')
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î")
            return df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î: {e}")
            logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return get_historical_disk_usage(days, 'test')

def train_forecast_model(df):
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ date, used_gb
    
    Returns:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        metrics: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        last_day: –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –≤ –¥–∞–Ω–Ω—ã—Ö
    """
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –¥–Ω–∏ –æ—Ç –Ω–∞—á–∞–ª–∞ –æ—Ç—Å—á–µ—Ç–∞
    df = df.sort_values('date').copy()
    df['day_num'] = (df['date'] - df['date'].min()).dt.days
    
    X = df['day_num'].values.reshape(-1, 1)
    y = df['used_gb'].values
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    metrics = {
        'mae': mae,
        'r2': r2,
        'growth_rate': model.coef_[0],
        'intercept': model.intercept_
    }
    
    logger.info(f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
    logger.info(f"  MAE: {mae:.2f} –ì–ë")
    logger.info(f"  R2: {r2:.3f}")
    logger.info(f"  –°–∫–æ—Ä–æ—Å—Ç—å —Ä–æ—Å—Ç–∞: {model.coef_[0]:.3f} –ì–ë/–¥–µ–Ω—å")
    logger.info(f"  –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {model.intercept_:.2f} –ì–ë")
    
    return model, metrics, df['day_num'].max()

def make_forecast(model, last_day, days_ahead):
    """
    –î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ days_ahead –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥
    """
    future_days = np.array([last_day + i for i in range(1, days_ahead + 1)]).reshape(-1, 1)
    forecast = model.predict(future_days)
    return forecast

def calculate_days_to_limit(model, current_usage, limit_gb):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞
    """
    if model.coef_[0] <= 0:
        return float('inf')  # –¥–∏—Å–∫ –Ω–µ —Ä–∞—Å—Ç–µ—Ç –∏–ª–∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è
    
    days_to_limit = (limit_gb - current_usage) / model.coef_[0]
    return max(0, days_to_limit)

def save_forecast_to_db(forecast_date, actual_gb, forecasts_dict, metrics):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –≤ PostgreSQL
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        # –í—Å—Ç–∞–≤–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
        query = """
            INSERT INTO disk_forecast 
                (metric_date, disk_used_gb, forecast_7d_gb, forecast_14d_gb, 
                 forecast_30d_gb, forecast_date, growth_rate_gb_per_day, days_to_limit)
            VALUES (%s, %s, %s, %s, %s, NOW(), %s, %s)
            ON CONFLICT (metric_date) 
            DO UPDATE SET 
                disk_used_gb = EXCLUDED.disk_used_gb,
                forecast_7d_gb = EXCLUDED.forecast_7d_gb,
                forecast_14d_gb = EXCLUDED.forecast_14d_gb,
                forecast_30d_gb = EXCLUDED.forecast_30d_gb,
                forecast_date = EXCLUDED.forecast_date,
                growth_rate_gb_per_day = EXCLUDED.growth_rate_gb_per_day,
                days_to_limit = EXCLUDED.days_to_limit
        """
        
        days_to_limit = calculate_days_to_limit(
            None, actual_gb, DISK_LIMIT_GB
        )  # –ó–¥–µ—Å—å –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å, —É–ø—Ä–æ—Å—Ç–∏–º –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        
        cur.execute(query, (
            forecast_date.date(),
            actual_gb,
            forecasts_dict.get(7),
            forecasts_dict.get(14),
            forecasts_dict.get(30),
            metrics['growth_rate'],
            days_to_limit
        ))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        cur.execute("""
            INSERT INTO model_quality (train_date, model_type, mae, r2, growth_rate)
            VALUES (NOW(), 'linear_regression', %s, %s, %s)
        """, (metrics['mae'], metrics['r2'], metrics['growth_rate']))
        
        conn.commit()
        cur.close()
        conn.close()
        logger.info(f"–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {forecast_date.date()} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
        raise

def check_critical_threshold(forecasts, current_usage):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏ –ø—Ä–æ–≥–Ω–æ–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥
    
    Returns:
        list: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    """
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
    for days, value in forecasts.items():
        if value > DISK_LIMIT_GB:
            warnings.append({
                'type': 'forecast',
                'days': days,
                'value': float(value),
                'threshold': DISK_LIMIT_GB,
                'message': f"–ß–µ—Ä–µ–∑ {days} –¥–Ω–µ–π –¥–∏—Å–∫ –ø—Ä–µ–≤—ã—Å–∏—Ç {DISK_LIMIT_GB} –ì–ë"
            })
            logger.warning(f"‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –ü–†–û–ì–ù–û–ó: —á–µ—Ä–µ–∑ {days} –¥–Ω–µ–π {value:.1f} –ì–ë")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
    if current_usage > DISK_LIMIT_GB * 0.9:
        warnings.append({
            'type': 'current',
            'value': float(current_usage),
            'threshold': DISK_LIMIT_GB,
            'message': f"–¢–µ–∫—É—â–µ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ {current_usage:.1f} –ì–ë (>90% –ª–∏–º–∏—Ç–∞)"
        })
        logger.warning(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: —Ç–µ–∫—É—â–µ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ {current_usage:.1f} –ì–ë")
    
    return warnings

def send_alerts(warnings):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞–ª–µ—Ä—Ç—ã (–≤—ã–∑–æ–≤ –≤–Ω–µ—à–Ω–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)
    """
    if not warnings:
        return
    
    try:
        from alert_telegram import send_telegram_alert
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = f"üö® **–ü–†–û–ì–ù–û–ó –ó–ê–ü–û–õ–ù–ï–ù–ò–Ø –î–ò–°–ö–ê {DISK_LETTER}**\n\n"
        for w in warnings:
            message += f"‚Ä¢ {w['message']}\n"
        
        message += f"\n–î–∞—Ç–∞ —Ä–∞—Å—á–µ—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
        send_telegram_alert(message)
        logger.info("–ê–ª–µ—Ä—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã")
        
    except ImportError:
        logger.warning("–ú–æ–¥—É–ª—å alert_telegram –Ω–µ –Ω–∞–π–¥–µ–Ω, –∞–ª–µ—Ä—Ç—ã –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤: {e}")

def main():
    parser = argparse.ArgumentParser(description='–ü—Ä–æ–≥–Ω–æ–∑ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∏—Å–∫–∞')
    parser.add_argument('--source', default='auto', 
                       choices=['auto', 'test', 'prometheus', 'windows'],
                       help='–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--days', type=int, default=60,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏')
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("–ó–ê–ü–£–°–ö –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –î–ò–°–ö–ê")
    logger.info(f"–î–∏—Å–∫: {DISK_LETTER}, –ª–∏–º–∏—Ç: {DISK_LIMIT_GB} –ì–ë")
    
    try:
        # 1. –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        df = get_historical_disk_usage(days=args.days, source=args.source)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π —Å {df['date'].min().date()} –ø–æ {df['date'].max().date()}")
        
        # 2. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model, metrics, last_day = train_forecast_model(df)
        
        # 3. –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
        forecasts = {}
        for days in FORECAST_DAYS:
            forecast_values = make_forecast(model, last_day, days)
            forecasts[days] = forecast_values[-1]  # –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –ø—Ä–æ–≥–Ω–æ–∑–∞
            logger.info(f"–ü—Ä–æ–≥–Ω–æ–∑ —á–µ—Ä–µ–∑ {days} –¥–Ω–µ–π: {forecast_values[-1]:.1f} –ì–ë")
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏
        current_usage = df['used_gb'].iloc[-1]
        warnings = check_critical_threshold(forecasts, current_usage)
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        save_forecast_to_db(df['date'].max(), current_usage, forecasts, metrics)
        
        # 6. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞–ª–µ—Ä—Ç—ã
        if warnings:
            send_alerts(warnings)
        else:
            logger.info("‚úÖ –í—Å–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã")
        
        logger.info("–ü–†–û–ì–ù–û–ó –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù")
        
    except Exception as e:
        logger.error(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        sys.exit(1)
    
    logger.info("=" * 60)

if __name__ == "__main__":
    main()
